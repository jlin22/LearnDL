{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    In this jupyter notebook, we are going to implement the various optimizers and regularizers.\n",
    "    \n",
    "    Let's start with importing the libraries that we're going to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Let's write a neural network model for 2 hidden layers.\n",
    "      \n",
    "    First we want to write a function that initializes the parameters W1, b1, W2, b2 randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' inputs: n_x (input feature dim), n_h (num of hidden layer units), n_y (num of output units)'''\n",
    "def two_layer_init(n_x, n_h, n_y):\n",
    "    \n",
    "    # initialize W's with gaussian random variables with variance 0.01\n",
    "    # initialize b's with zeros, because they don't need symmetry broken\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    # store the weights in a dictionary for easy access in future uses of the weights\n",
    "    parameters = {\n",
    "        'W1': W1,\n",
    "        'b1': b1,\n",
    "        'W2': W2,\n",
    "        'b2': b2\n",
    "    }\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now let's test our two layer init function with small dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "[[-0.00653288 -0.00819667  0.00031673  0.00889022  0.01301598]\n",
      " [ 0.00206776  0.0089669   0.0127582  -0.00828307 -0.00073103]\n",
      " [-0.0018422   0.00376808 -0.01355366  0.01301257  0.0128692 ]\n",
      " [ 0.01187923 -0.0020258  -0.00485629  0.01572953 -0.00682283]]\n",
      "b1\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2\n",
      "[[ 6.17140506e-03  5.63390061e-03  6.90247743e-03 -7.49576961e-03]\n",
      " [-9.64550975e-03  6.91329132e-03 -1.47138253e-02  1.79246889e-02]\n",
      " [-2.12096969e-03  1.15474087e-02 -1.95564450e-02  7.49957548e-05]]\n",
      "b2\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W1 True\n",
      "b1 True\n",
      "W2 True\n",
      "b2 True\n"
     ]
    }
   ],
   "source": [
    "parameters = two_layer_init(5, 4, 3)\n",
    "n_x, n_h, n_y = 5, 4, 3\n",
    "# check that we use zeros for b's and randn for W's\n",
    "for key, value in parameters.items():\n",
    "    print(key)\n",
    "    print(value)\n",
    "\n",
    "# check shapes are correct\n",
    "right_dims = [(n_h, n_x), (n_h, 1), (n_y, n_h), (n_y, 1)]\n",
    "i = 0\n",
    "for key, value in parameters.items():\n",
    "    print(key, str(right_dims[i] == value.shape))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Let's now write a function to do forward propagation. \n",
    "    \n",
    "    We're going to use ReLU for the hidden layer and sigmoid for the output layer.\n",
    "    \n",
    "    We also need to write a small ReLU and sigmoid function right before our forward propagation implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(Z, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(100, 100)\n",
    "print(X.shape)\n",
    "Z = relu(X)\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We're going to need to save a cache for backward propagation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_forward_propagation(X, parameters):\n",
    "    \n",
    "    # for backward propagation\n",
    "    cache = {}\n",
    "    \n",
    "    Z1 = np.dot(parameters['W1'], X) + parameters['b1']\n",
    "    A1 = relu(Z1)\n",
    "    \n",
    "    Z2 = np.dot(parameters['W2'], Z1) + parameters['b2']\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    # turns out you don't need the last layers A and Z, but you do need X\n",
    "    # you also need everything in the hidden layers\n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We're almost at writing backward propagation. We need to write the function for computing cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    return -1 / m * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now let's write the code for backward propagation.\n",
    "    \n",
    "    However, before that, we need to write a function that calculates the relu derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(Z):\n",
    "    grad = np.where(Z >= 0, 1, 0)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_derivative(-1): 0\n",
      "relu_derivative(1): 1\n"
     ]
    }
   ],
   "source": [
    "print(\"relu_derivative(-1): %d\" % relu_derivative(-1))\n",
    "print(\"relu_derivative(1): %d\" % relu_derivative(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_backward_propagation(A2, Y, X, parameters, cache, learning_rate = 0.1):\n",
    "    \n",
    "    # we should get the things we need from fprop's cache now\n",
    "    A1 = cache['A1']\n",
    "    Z1 = cache['Z1']\n",
    "    W1, b1, W2, b2 = parameters.values()\n",
    "    \n",
    "    # calculate the gradients\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * relu_derivative(Z1)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    # now that we have the gradients, let's perform gradient descent\n",
    "    parameters['W1'] = parameters['W1'] - learning_rate * dW1\n",
    "    parameters['b1'] = parameters['b1'] - learning_rate * db1\n",
    "    parameters['W2'] = parameters['W2'] - learning_rate * dW2\n",
    "    parameters['b2'] = parameters['b2'] - learning_rate * db2\n",
    "    \n",
    "    # what do we return?\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We're done with backward propagation now. \n",
    "    \n",
    "    Let's write the function accuracy, because that is the metric that we care about.\n",
    "    \n",
    "    Cost is important, but accuracy is the most important for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(Y_pred, Y):\n",
    "    print(Y_pred * Y)\n",
    "    accuracy = np.mean(Y_pred * Y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 1 1 1 0 1 0] [1 1 1 0 1 1 0 0 0 1]\n",
      "[0 0 1 0 1 1 0 0 0 0]\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "a = np.array([np.random.randint(0, 2) for i in range(10)])\n",
    "b = np.array([np.random.randint(0, 2) for i in range(10)])\n",
    "print(a, b)\n",
    "print(compute_accuracy(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We also need a function for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(A2):\n",
    "    return A2 >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X_train, Y_train, X_test, Y_test, learning_rate = 0.1, num_epochs = 101):\n",
    "\n",
    "    n_x, m = X_train.shape\n",
    "    n_y, _ = Y_train.shape\n",
    "    n_h = 6\n",
    "    \n",
    "    # initialize variables\n",
    "    parameters = two_layer_init(n_x, n_h, n_y)\n",
    "    \n",
    "    # initialize list of cost and accuracy\n",
    "    costs = []\n",
    "    training_accuracies = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # forward propagation\n",
    "        A2, cache = two_layer_forward_propagation(X_train, parameters)\n",
    "\n",
    "        # compute cost\n",
    "        cost = compute_cost(A2, Y_train)\n",
    "\n",
    "        # backward_propagation\n",
    "        parameters = two_layer_backward_propagation(A2, Y_train, X_train, parameters, cache, learning_rate = learning_rate)\n",
    "        \n",
    "        # every 100 epochs, add cost to the list\n",
    "        if epoch % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "            training_predictions = predict(A2)\n",
    "            training_accuracy = compute_accuracy(training_predictions, Y_train)\n",
    "            training_accuracies.append(training_accuracy)\n",
    "            \n",
    "            \n",
    "            \n",
    "    return parameters, training_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    We need a dataset to break our code on.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150, 1)\n"
     ]
    }
   ],
   "source": [
    "data = sklearn.datasets.load_iris()\n",
    "X = data['data']\n",
    "Y = data['target']\n",
    "Y = Y.reshape((Y.shape[0], 1))\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now, we have the iris dataset.\n",
    "    \n",
    "    We should split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 150) (1, 150)\n"
     ]
    }
   ],
   "source": [
    "permutation = np.random.permutation(X.shape[0])\n",
    "X = X[permutation]\n",
    "Y = Y[permutation]\n",
    "X = X.T\n",
    "Y = Y.T\n",
    "print(X.shape, Y.shape)\n",
    "X_train, X_test = X[:, 0:100], X[:, 100:]\n",
    "Y_train, Y_test = Y[:, 0:100], Y[:, 100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100) (4, 50) (1, 100) (1, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 0 0 1 0 1 1 1 2 1 1 2 2 0 0 1 2 2 1 2 0 0 0 2 1 1 2 2 2 0 1 1 0 0\n",
      "  0 2 2 2 1 2 2 1 2 1 1 0 0 0 0 2 2 2 1 0 0 1 2 2 0 2 2 2 0 1 1 2 0 0 2 2\n",
      "  1 1 0 1 2 1 0 2 2 2 1 2 0 2 2 0 0 1 1 1 1 2 2 2 0 1 1 0]]\n",
      "[[0 1 2 0 0 1 0 1 1 1 2 1 1 2 2 0 0 1 2 2 1 2 0 0 0 2 1 1 2 2 2 0 1 1 0 0\n",
      "  0 2 2 2 1 2 2 1 2 1 1 0 0 0 0 2 2 2 1 0 0 1 2 2 0 2 2 2 0 1 1 2 0 0 2 2\n",
      "  1 1 0 1 2 1 0 2 2 2 1 2 0 2 2 0 0 1 1 1 1 2 2 2 0 1 1 0]]\n",
      "[1.08, 1.08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jlin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/jlin/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:32: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_sum(a, axis, dtype, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "parameters, training_accuracies = two_layer_model(X_train, Y_train, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now we should generalize this to n layer neural networks. \n",
    "    \n",
    "    We're going to need to rewrite initialize parameters.\n",
    "   \n",
    "    Let's also use He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def he_initialization(layer_dims):\n",
    "    '''\n",
    "    input: layer_dims\n",
    "    (n_x, n_1, n_2, ..., n_L)\n",
    "    len = L + 1\n",
    "    output: parameters\n",
    "    {'W1', 'b1', 'W2', 'b2', ..., 'WL', 'bL'}\n",
    "    weights : 1 -> L\n",
    "    '''\n",
    "    \n",
    "    L = len(layer_dims) - 1\n",
    "    parameters = {}\n",
    "    \n",
    "    for l in range(1, L + 1): # we want to include L\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2.0 / layer_dims[l - 1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "layer_dims = (100, 50, 6, 1)\n",
    "parameters = he_initialization(layer_dims)\n",
    "print(len(parameters) // 2)\n",
    "#print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Now let's write a function to do forward propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    '''inputs: X, parameters\n",
    "    outputs: AL, cache\n",
    "    cache contains A, Z for layers 1:L for backprop\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    cache = {}\n",
    "    \n",
    "    A = X\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        Z = np.dot(parameters['W' + str(l)], A) + parameters['b' + str(l)]\n",
    "        A = relu(Z)\n",
    "        cache['Z' + str(l)] = Z \n",
    "        cache['A' + str(l)] = A\n",
    "        # use relu for every layer but the last\n",
    "        \n",
    "    # last layer\n",
    "    Z = np.dot(parameters['W' + str(L)], A) + parameters['b' + str(L)]\n",
    "    A = sigmoid(Z)\n",
    "    # dont add the Z and A for last layer, because they're not necessary for the backward propagation calculation\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.rand(100, 100)\n",
    "A, cache = forward_propagation(X, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
