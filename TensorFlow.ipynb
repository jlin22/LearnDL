{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play around with variables in tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# use tf.constant for constants\n",
    "x = tf.constant(100, name = 'x')\n",
    "y = tf.constant(200, name = 'x')\n",
    "\n",
    "# use tf.Variable for things that are variables (can change)\n",
    "dist = tf.Variable(abs(x - y), name = 'dist')\n",
    "\n",
    "# create your global variables initializer\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to create variables, constants and the global variables initializer. You also know how to create a session, initialize your global variables, and run a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing a Tensorflow program consists of creating variables and defining operations between them, initializing your variables, then creating and running your session on your variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play with placeholders. You can create placeholders with `tf.placeholder`. When you run operations with a placeholder, you need a feed dictionary, which contains the values of those placeholders. Let's write the code above with placeholders now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, name = 'x')\n",
    "y = tf.placeholder(tf.float32, name = 'y')\n",
    "\n",
    "d = abs(x - y)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "xval = 100\n",
    "yval = 200\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(d, feed_dict = {x: xval, y : yval}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the sigmoid function with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    x = tf.placeholder(tf.float32, name = 'x')\n",
    "    \n",
    "    sigmoid = tf.sigmoid(x)\n",
    "    with tf.Session() as sess:\n",
    "        a = sess.run(sigmoid, feed_dict = {x: z})\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(0))\n",
    "print(sigmoid(float('inf')))\n",
    "print(sigmoid(float('-inf')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred, y_true):\n",
    "    Y = tf.placeholder(tf.float32, name = 'Y')\n",
    "    A = tf.placeholder(tf.float32, name = 'A')\n",
    "    \n",
    "    loss = -(tf.multiply(Y, tf.log(A)) + tf.multiply(1 - Y, tf.log(1 - A)))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        l = sess.run(loss, feed_dict = {Y: y_true, A: y_pred})\n",
    "        \n",
    "    return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2231435513142097\n",
      "0.22314353\n"
     ]
    }
   ],
   "source": [
    "print(-1 * np.log(0.8))\n",
    "print(loss(0.8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note is that you can use `tf.nn.sigmoid_cross_entropy_with_logits(logits = , labels = )`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should implement one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, C):\n",
    "    C = tf.constant(C, name = 'C')\n",
    "    one_hot_matrix = tf.one_hot(labels, C, axis = 0)\n",
    "    with tf.Session() as sess:\n",
    "        one_hot = sess.run(one_hot_matrix)\n",
    "        \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = np.array([1, 2, 3, 1, 0])\n",
    "C = 4\n",
    "print(one_hot(labels, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what have we learned about so far. We've learned about `tf.sigmoid, tf.nn.sigmoid_cross_entropy_with_logits, tf.one_hot`. Now we're going to learn about `tf.ones`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ones(shape):\n",
    "    ones = tf.ones(shape)\n",
    "    with tf.Session() as sess:\n",
    "        out = sess.run(ones)\n",
    "        \n",
    "    return out    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(ones((5, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make a function for creating placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder('float', (n_x, None))\n",
    "    Y = tf.placeholder('float', (n_y, None))\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, ?)\n",
      "X.shape = (100, ?)\n",
      "Y.shape = (4, ?)\n"
     ]
    }
   ],
   "source": [
    "X, Y = create_placeholders(100, 4)\n",
    "print('X.shape = ' + str(X.shape))\n",
    "print('Y.shape = ' + str(Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a function for initializing parameters. We're going to use a 3 layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_y):\n",
    "    W1 = tf.get_variable('W1', shape = (25, n_x), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', shape = (25, 1))\n",
    "    W2 = tf.get_variable('W2', shape = (12, 25), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', shape = (12, 1))\n",
    "    W3 = tf.get_variable('W3', shape = (n_x, 12), initializer = tf.contrib.layers.xavier_initializer())\n",
    "    b3 = tf.get_variable('b3', shape = (n_y, 1))\n",
    "    \n",
    "    parameters = {'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3': W3, 'b3': b3}\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1(25, 100)\n",
      "b1(25, 1)\n",
      "W2(12, 25)\n",
      "b2(12, 1)\n",
      "W3(100, 12)\n",
      "b3(4, 1)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "parameters = initialize_parameters(100, 4)\n",
    "for k, v in parameters.items():\n",
    "    print(k + str(v.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's implement the forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' X is the placeholder for the input \n",
    "    return: Z3'''\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    Z1 = tf.matmul(parameters['W1'], X) + parameters['b1']\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.matmul(parameters['W2'], A1) + parameters['b2']\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.matmul(parameters['W3'], A2) + parameters['b3']\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a compute cost function. After this, we can easily write our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Z3, Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use all the functions we've written and create our neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
